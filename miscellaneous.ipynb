{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b2aef1",
   "metadata": {},
   "source": [
    "Add estimated filing dates to OpenSecrets.org records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1933f019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Completed expenditures20.csv: 131,457,097 rows.                 \n",
      "Processing expenditures22.csv...\n",
      "   Completed expenditures22.csv: 55,463,229 rows.                 \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "from datetime import timedelta\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "DATA_ROOT = 'data/raw/campaign_finance_open_secrets'\n",
    "CHUNK_SIZE = 100000\n",
    "\n",
    "csv.field_size_limit(2147483647)\n",
    "\n",
    "ELECTION_DATES = {\n",
    "    2012: pd.Timestamp('2012-11-06'),\n",
    "    2014: pd.Timestamp('2014-11-04'),\n",
    "    2016: pd.Timestamp('2016-11-08'),\n",
    "    2018: pd.Timestamp('2018-11-06'),\n",
    "    2020: pd.Timestamp('2020-11-03'),\n",
    "    2022: pd.Timestamp('2022-11-08')\n",
    "}\n",
    "\n",
    "def load_hs_filer_map(root):\n",
    "    print(\"Building House/Senate Filer Map...\")\n",
    "    hs_cmte_ids = set()\n",
    "    hs_cand_ids = set()\n",
    "\n",
    "    cands_files = glob.glob(os.path.join(root, 'cands*.csv'))\n",
    "    for f in cands_files:\n",
    "        try:\n",
    "            for chunk in pd.read_csv(f, usecols=['FECCandID', 'DistIDRunFor'], \n",
    "                                     dtype=str, on_bad_lines='skip', chunksize=50000):\n",
    "                mask = chunk['DistIDRunFor'].notna() & (chunk['DistIDRunFor'] != 'US')\n",
    "                hs_cand_ids.update(chunk.loc[mask, 'FECCandID'].unique())\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    cmtes_files = glob.glob(os.path.join(root, 'cmtes*.csv'))\n",
    "    for f in cmtes_files:\n",
    "        try:\n",
    "            for chunk in pd.read_csv(f, usecols=['CmteID', 'FECCandID'], \n",
    "                                     dtype=str, on_bad_lines='skip', chunksize=50000):\n",
    "                mask = chunk['FECCandID'].isin(hs_cand_ids)\n",
    "                hs_cmte_ids.update(chunk.loc[mask, 'CmteID'].unique())\n",
    "        except Exception as e:\n",
    "            pass\n",
    "            \n",
    "    print(f\"   Identified {len(hs_cand_ids)} H/S Candidates and {len(hs_cmte_ids)} Committees.\")\n",
    "    return hs_cmte_ids, hs_cand_ids\n",
    "\n",
    "def get_col_name(df, target):\n",
    "    \"\"\"Finds a column name case-insensitively to prevent KeyErrors.\"\"\"\n",
    "    for col in df.columns:\n",
    "        if col.lower() == target.lower():\n",
    "            return col\n",
    "    return target\n",
    "\n",
    "def calculate_dates_in_memory(df, cycle, hs_cmte_ids, hs_cand_ids, table_type):\n",
    "    df['estimated_filing_date'] = pd.NaT\n",
    "    \n",
    "    date_col = get_col_name(df, 'Date')\n",
    "    df['temp_date'] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "\n",
    "    # 1. Determine Filer Type safely using case-insensitive column lookups\n",
    "    if table_type == 'indivs' or table_type == 'expenditures':\n",
    "        cmte_col = get_col_name(df, 'CmteID')\n",
    "        is_quarterly = df[cmte_col].isin(hs_cmte_ids) if cmte_col in df.columns else pd.Series(False, index=df.index)\n",
    "    elif table_type == 'pacs':\n",
    "        cand_col = get_col_name(df, 'FECCandID')\n",
    "        is_quarterly = df[cand_col].isin(hs_cand_ids) if cand_col in df.columns else pd.Series(False, index=df.index)\n",
    "    elif table_type == 'pac_other':\n",
    "        filer_col = get_col_name(df, 'FilerID')\n",
    "        is_quarterly = df[filer_col].isin(hs_cmte_ids) if filer_col in df.columns else pd.Series(False, index=df.index)\n",
    "    else:\n",
    "        is_quarterly = pd.Series(False, index=df.index)\n",
    "\n",
    "    # 2. Apply 48-Hour Rule\n",
    "    election_date = ELECTION_DATES.get(int(cycle))\n",
    "    if election_date:\n",
    "        start_20_day = election_date - timedelta(days=20)\n",
    "        mask_48h = (df['temp_date'] >= start_20_day) & (df['temp_date'] < election_date)\n",
    "        df.loc[mask_48h, 'estimated_filing_date'] = df.loc[mask_48h, 'temp_date'] + timedelta(days=2)\n",
    "        mask_regular = ~mask_48h\n",
    "    else:\n",
    "        mask_regular = pd.Series(True, index=df.index)\n",
    "\n",
    "    # 3. Apply Quarterly Logic\n",
    "    mask_q = mask_regular & is_quarterly & df['temp_date'].notna()\n",
    "    if mask_q.any():\n",
    "        dates_q = df.loc[mask_q, 'temp_date']\n",
    "        months = dates_q.dt.month\n",
    "        years = dates_q.dt.year\n",
    "        \n",
    "        target_year = years.copy()\n",
    "        target_month = pd.Series(0, index=dates_q.index)\n",
    "        target_day = pd.Series(0, index=dates_q.index)\n",
    "        \n",
    "        m1 = (months >= 1) & (months <= 3)\n",
    "        target_month[m1], target_day[m1] = 4, 15\n",
    "        \n",
    "        m2 = (months >= 4) & (months <= 6)\n",
    "        target_month[m2], target_day[m2] = 7, 15\n",
    "        \n",
    "        m3 = (months >= 7) & (months <= 9)\n",
    "        target_month[m3], target_day[m3] = 10, 15\n",
    "        \n",
    "        m4 = (months >= 10) & (months <= 12)\n",
    "        target_month[m4], target_day[m4] = 1, 31\n",
    "        target_year[m4] += 1\n",
    "        \n",
    "        df.loc[mask_q, 'estimated_filing_date'] = pd.to_datetime({\n",
    "            'year': target_year, 'month': target_month, 'day': target_day\n",
    "        })\n",
    "\n",
    "    # 4. Apply Monthly Logic\n",
    "    mask_m = mask_regular & (~is_quarterly) & df['temp_date'].notna()\n",
    "    if mask_m.any():\n",
    "        dates_m = df.loc[mask_m, 'temp_date']\n",
    "        months = dates_m.dt.month\n",
    "        years = dates_m.dt.year\n",
    "        \n",
    "        target_year_m = years.copy()\n",
    "        target_month_m = months + 1\n",
    "        target_day_m = pd.Series(20, index=dates_m.index)\n",
    "        \n",
    "        is_dec = (months == 12)\n",
    "        target_month_m[is_dec], target_day_m[is_dec] = 1, 31\n",
    "        target_year_m[is_dec] += 1\n",
    "        \n",
    "        df.loc[mask_m, 'estimated_filing_date'] = pd.to_datetime({\n",
    "            'year': target_year_m, 'month': target_month_m, 'day': target_day_m\n",
    "        })\n",
    "\n",
    "    df.drop(columns=['temp_date'], inplace=True)\n",
    "    return df\n",
    "\n",
    "def process_file_safe(filepath, cycle, hs_cmte_ids, hs_cand_ids, table_type):\n",
    "    temp_file = filepath + '.tmp'\n",
    "    chunk_buffer = []\n",
    "    row_count = 0\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8', errors='replace') as f_in:\n",
    "            reader = csv.reader(f_in)\n",
    "            \n",
    "            try:\n",
    "                # Strip invisible characters and spaces from headers\n",
    "                raw_header = next(reader)\n",
    "                header = [str(h).strip('\\ufeff').strip() for h in raw_header]\n",
    "                expected_cols = len(header)\n",
    "            except StopIteration:\n",
    "                print(f\"Skipping empty file: {filepath}\")\n",
    "                return\n",
    "\n",
    "            with open(temp_file, 'w', encoding='utf-8', newline='') as f_out:\n",
    "                for row in reader:\n",
    "                    # FIX: Force rows to perfectly match the header length\n",
    "                    if len(row) > expected_cols:\n",
    "                        row = row[:expected_cols]\n",
    "                    elif len(row) < expected_cols:\n",
    "                        row.extend([''] * (expected_cols - len(row)))\n",
    "                        \n",
    "                    chunk_buffer.append(row)\n",
    "                    \n",
    "                    if len(chunk_buffer) >= CHUNK_SIZE:\n",
    "                        df_chunk = pd.DataFrame(chunk_buffer, columns=header)\n",
    "                        df_chunk = calculate_dates_in_memory(df_chunk, cycle, hs_cmte_ids, hs_cand_ids, table_type)\n",
    "                        \n",
    "                        write_header = (row_count == 0)\n",
    "                        df_chunk.to_csv(f_out, index=False, header=write_header)\n",
    "                        \n",
    "                        row_count += len(df_chunk)\n",
    "                        chunk_buffer = [] \n",
    "                        print(f\"   Processed {row_count:,} rows...\", end='\\r')\n",
    "\n",
    "                if chunk_buffer:\n",
    "                    df_chunk = pd.DataFrame(chunk_buffer, columns=header)\n",
    "                    df_chunk = calculate_dates_in_memory(df_chunk, cycle, hs_cmte_ids, hs_cand_ids, table_type)\n",
    "                    write_header = (row_count == 0)\n",
    "                    df_chunk.to_csv(f_out, index=False, header=write_header)\n",
    "                    row_count += len(df_chunk)\n",
    "\n",
    "        print(f\"   Completed {os.path.basename(filepath)}: {row_count:,} rows.                 \")\n",
    "        os.replace(temp_file, filepath)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ FAILED on {filepath}: {e}\")\n",
    "        if os.path.exists(temp_file):\n",
    "            os.remove(temp_file)\n",
    "\n",
    "def main():\n",
    "    hs_cmte_ids, hs_cand_ids = load_hs_filer_map(DATA_ROOT)\n",
    "    \n",
    "    target_tables = ['indivs', 'pacs', 'pac_other', 'expenditures']\n",
    "    \n",
    "    for table in target_tables:\n",
    "        pattern = os.path.join(DATA_ROOT, f'{table}*.csv')\n",
    "        files = glob.glob(pattern)\n",
    "        \n",
    "        for f in files:\n",
    "            print(f\"Processing {os.path.basename(f)}...\")\n",
    "            try:\n",
    "                cycle_short = os.path.basename(f).replace(table, '').replace('.csv', '')\n",
    "                cycle = int('20' + cycle_short)\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "            process_file_safe(f, cycle, hs_cmte_ids, hs_cand_ids, table)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b008ee6b",
   "metadata": {},
   "source": [
    "Add estimated filing dates to lobbying records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9270d889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reports.csv updated successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def process_lobbying_reports():\n",
    "  filepath = os.path.join('data', 'raw', 'lobbying_data_lobbyview', 'reports.csv')\n",
    "  temp_file = filepath + '.tmp'\n",
    "  chunk_size = 100000\n",
    "\n",
    "  if not os.path.exists(filepath):\n",
    "      print(\"File not found.\")\n",
    "      return\n",
    "\n",
    "  # Map each filing period code to the mm-dd string of its final day\n",
    "  period_end_dates = {\n",
    "      'Q1': '03-31',\n",
    "      'Q2': '06-30',\n",
    "      'Q3': '09-30',\n",
    "      'Q4': '12-31',\n",
    "      'H1': '06-30',  # Older pre-2008 semi-annual filings\n",
    "      'H2': '12-31'\n",
    "  }\n",
    "\n",
    "  first_chunk = True\n",
    "  for chunk in pd.read_csv(filepath, dtype=str, chunksize=chunk_size):\n",
    "      chunk['estimated_filing_date'] = pd.NaT\n",
    "\n",
    "      if 'filing_year' in chunk.columns and 'filing_period_code' in chunk.columns:\n",
    "          valid_rows = chunk['filing_year'].notna() & chunk['filing_period_code'].notna()\n",
    "          \n",
    "          period_codes = chunk.loc[valid_rows, 'filing_period_code'].str.strip().str.upper()\n",
    "          mapped_dates = period_codes.map(period_end_dates)\n",
    "          \n",
    "          valid_mapped = valid_rows & mapped_dates.notna()\n",
    "          \n",
    "          # Combine year and mm-dd to build a parseable date string\n",
    "          date_strings = chunk.loc[valid_mapped, 'filing_year'].astype(str).str.replace('.0', '', regex=False) + '-' + mapped_dates[valid_mapped]\n",
    "          \n",
    "          # Convert to datetime and add the 20 day filing window\n",
    "          chunk.loc[valid_mapped, 'estimated_filing_date'] = pd.to_datetime(date_strings, errors='coerce') + pd.Timedelta(days=20)\n",
    "\n",
    "      chunk.to_csv(temp_file, mode='w' if first_chunk else 'a', index=False, header=first_chunk)\n",
    "      first_chunk = False\n",
    "\n",
    "  os.replace(temp_file, filepath)\n",
    "  print(\"reports.csv updated successfully.\")\n",
    "  \n",
    "process_lobbying_reports()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6dba80",
   "metadata": {},
   "source": [
    "Add estimated filing dates to 527 transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d022a350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f77ca8f2e54a63bfcaba6047c41716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rcpts527.csv:   0%|          | 0/4715027 [00:00<?, ?rows/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ rcpts527.csv updated successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Mute Pandas warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='pandas')\n",
    "\n",
    "DATA_ROOT = os.path.join('data', 'raw', '527_data_open_secrets')\n",
    "CHUNK_SIZE = 100000\n",
    "csv.field_size_limit(2147483647)\n",
    "\n",
    "def get_latest_filing_date(dates):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of the latest possible IRS Form 8872 filing date.\n",
    "    \"\"\"\n",
    "    latest_dates = pd.Series(pd.NaT, index=dates.index)\n",
    "    valid = dates.notna()\n",
    "    \n",
    "    if not valid.any():\n",
    "        return latest_dates\n",
    "        \n",
    "    y = dates.dt.year\n",
    "    m = dates.dt.month\n",
    "    \n",
    "    # 1. Monthly Schedule Deadlines\n",
    "    monthly_y = y.copy()\n",
    "    monthly_m = m + 1\n",
    "    monthly_d = pd.Series(20, index=dates.index)\n",
    "    \n",
    "    is_dec = (m == 12)\n",
    "    monthly_m[is_dec] = 1\n",
    "    monthly_d[is_dec] = 31\n",
    "    monthly_y[is_dec] += 1\n",
    "    \n",
    "    monthly_deadline = pd.to_datetime({\n",
    "        'year': monthly_y[valid], \n",
    "        'month': monthly_m[valid], \n",
    "        'day': monthly_d[valid]\n",
    "    })\n",
    "    \n",
    "    # 2. Non-Monthly Schedule Deadlines\n",
    "    period_y = y.copy()\n",
    "    period_m = pd.Series(0, index=dates.index)\n",
    "    period_d = pd.Series(0, index=dates.index)\n",
    "    \n",
    "    is_even_year = (y % 2 == 0)\n",
    "    is_odd_year = ~is_even_year\n",
    "    \n",
    "    # Odd Years: Semi-Annual\n",
    "    odd_h1 = is_odd_year & (m <= 6)\n",
    "    period_m[odd_h1], period_d[odd_h1] = 7, 31\n",
    "    \n",
    "    odd_h2 = is_odd_year & (m > 6)\n",
    "    period_m[odd_h2], period_d[odd_h2] = 1, 31\n",
    "    period_y[odd_h2] += 1\n",
    "    \n",
    "    # Even Years: Quarterly\n",
    "    even_q1 = is_even_year & (m <= 3)\n",
    "    period_m[even_q1], period_d[even_q1] = 4, 15\n",
    "    \n",
    "    even_q2 = is_even_year & (m >= 4) & (m <= 6)\n",
    "    period_m[even_q2], period_d[even_q2] = 7, 15\n",
    "    \n",
    "    even_q3 = is_even_year & (m >= 7) & (m <= 9)\n",
    "    period_m[even_q3], period_d[even_q3] = 10, 15\n",
    "    \n",
    "    even_q4 = is_even_year & (m >= 10)\n",
    "    period_m[even_q4], period_d[even_q4] = 1, 31\n",
    "    period_y[even_q4] += 1\n",
    "    \n",
    "    period_deadline = pd.to_datetime({\n",
    "        'year': period_y[valid], \n",
    "        'month': period_m[valid], \n",
    "        'day': period_d[valid]\n",
    "    })\n",
    "    \n",
    "    # 3. Take the maximum (latest) of the two legal deadlines\n",
    "    latest_dates[valid] = pd.DataFrame({\n",
    "        'monthly': monthly_deadline, \n",
    "        'period': period_deadline\n",
    "    }).max(axis=1)\n",
    "    \n",
    "    return latest_dates\n",
    "\n",
    "def get_line_count(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        return sum(1 for _ in f)\n",
    "\n",
    "def has_been_processed(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8', errors='replace') as f:\n",
    "            reader = csv.reader(f)\n",
    "            header = next(reader)\n",
    "            return any('estimated_filing_date' in str(h).lower() for h in header)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def process_527_file(filename):\n",
    "    filepath = os.path.join(DATA_ROOT, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"File not found: {filepath}\")\n",
    "        return\n",
    "\n",
    "    if has_been_processed(filepath):\n",
    "        print(f\"   ⏩ Skipping {filename} (already processed).\")\n",
    "        return\n",
    "\n",
    "    temp_file = filepath + '.tmp'\n",
    "    row_count = 0\n",
    "    total_rows = max(0, get_line_count(filepath) - 1)\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8', errors='replace') as f_in:\n",
    "            reader = csv.reader(f_in)\n",
    "            try:\n",
    "                raw_header = next(reader)\n",
    "                header = [str(h).strip('\\ufeff').strip() for h in raw_header]\n",
    "                expected_cols = len(header)\n",
    "            except StopIteration:\n",
    "                return\n",
    "\n",
    "            with open(temp_file, 'w', encoding='utf-8', newline='') as f_out:\n",
    "                chunk_buffer = []\n",
    "                with tqdm(total=total_rows, desc=filename, unit=\"rows\") as pbar:\n",
    "                    for row in reader:\n",
    "                        if len(row) > expected_cols:\n",
    "                            row = row[:expected_cols]\n",
    "                        elif len(row) < expected_cols:\n",
    "                            row.extend([''] * (expected_cols - len(row)))\n",
    "                            \n",
    "                        chunk_buffer.append(row)\n",
    "                        \n",
    "                        if len(chunk_buffer) >= CHUNK_SIZE:\n",
    "                            df_chunk = pd.DataFrame(chunk_buffer, columns=header)\n",
    "                            \n",
    "                            date_col = next((c for c in df_chunk.columns if c.lower() == 'date'), None)\n",
    "                            if date_col:\n",
    "                                df_chunk['temp_date'] = pd.to_datetime(df_chunk[date_col], errors='coerce')\n",
    "                                df_chunk['estimated_filing_date'] = get_latest_filing_date(df_chunk['temp_date'])\n",
    "                                df_chunk.drop(columns=['temp_date'], inplace=True)\n",
    "                            else:\n",
    "                                df_chunk['estimated_filing_date'] = pd.NaT\n",
    "                                \n",
    "                            write_header = (row_count == 0)\n",
    "                            df_chunk.to_csv(f_out, index=False, header=write_header)\n",
    "                            \n",
    "                            row_count += len(df_chunk)\n",
    "                            pbar.update(len(df_chunk))\n",
    "                            chunk_buffer = []\n",
    "\n",
    "                    if chunk_buffer:\n",
    "                        df_chunk = pd.DataFrame(chunk_buffer, columns=header)\n",
    "                        date_col = next((c for c in df_chunk.columns if c.lower() == 'date'), None)\n",
    "                        if date_col:\n",
    "                            df_chunk['temp_date'] = pd.to_datetime(df_chunk[date_col], errors='coerce')\n",
    "                            df_chunk['estimated_filing_date'] = get_latest_filing_date(df_chunk['temp_date'])\n",
    "                            df_chunk.drop(columns=['temp_date'], inplace=True)\n",
    "                        else:\n",
    "                            df_chunk['estimated_filing_date'] = pd.NaT\n",
    "                            \n",
    "                        write_header = (row_count == 0)\n",
    "                        df_chunk.to_csv(f_out, index=False, header=write_header)\n",
    "                        pbar.update(len(chunk_buffer))\n",
    "\n",
    "        os.replace(temp_file, filepath)\n",
    "        print(f\"✅ {filename} updated successfully.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ FAILED on {filepath}: {e}\")\n",
    "        if os.path.exists(temp_file):\n",
    "            os.remove(temp_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for f in ['rcpts527.csv']:\n",
    "        process_527_file(f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
