# Experiment Log

This file tracks all model training runs with their configurations, changes, and results.

---

## Format
Each entry includes:
- **Date/Time**: When the experiment was run
- **Changes Made**: Description of modifications
- **Configuration**: Key hyperparameters
- **Results**: Summary metrics
- **Notes**: Observations

---

## Experiments

### [Template]
```
Date: YYYY-MM-DD HH:MM
Changes: 
Config: epochs=X, batch=Y, lr=Z, loss=BCE
Results: AUC=0.XX, ACC=0.XX, F1=0.XX (on 2023)
Notes: 
```

---

### Experiment 001: Baseline Dynamic Labels
```
Date: 2024-12-14 16:20
Changes: 
  - Added dynamic label masking (resolved historical labels visible)
  - Added age feature for historical edges
  - Monthly retraining with 5 epochs
Config: 
  - epochs=5, batch=200, lr=0.0001, loss=BCE
  - target_years=[2021, 2022, 2023, 2024]
  - raw_msg_dim=3, edge_dim=time+3+2
Results (2023 only):
  - Best Month: 2023-07 AUC=0.808
  - Worst Month: 2023-01 AUC=0.423
  - Avg AUC: ~0.55, Avg F1: ~0.51
Notes:
  - High volatility across months
  - Model captures strong signal in some months but fails in others
  - Need to investigate 2023-07 success factors
```

---

### Experiment 002: Phase 1-2 Improvements (2023 Focus)
```
Date: 2024-12-14 17:00
Changes:
  - Focus on 2023 only (12 months)
  - Added validation split (90/10 chronological)
  - max_epochs=20 with early stopping (patience=5)
  - Added PR-AUC and Macro-F1 metrics
  - Best model checkpoint loading
  
Config:
  - epochs=20 (max), patience=5
  - batch=200, lr=0.0001
  - loss=BCE (unweighted)
  - validation=10% of train (chronological)
  
Results (2023):
  | Month | ROC-AUC | PR-AUC | ACC   | F1    | Macro-F1 | Count |
  |-------|---------|--------|-------|-------|----------|-------|
  | Jan   | 0.376   | 0.492  | 0.426 | 0.478 | 0.420    | 331   |
  | Feb   | 0.539   | 0.545  | 0.570 | 0.434 | 0.543    | 230   |
  | Mar   | 0.565   | 0.596  | 0.520 | 0.431 | 0.508    | 342   |
  | Apr   | 0.498   | 0.543  | 0.519 | 0.647 | 0.446    | 370   |
  | May   | 0.609   | 0.710  | 0.576 | 0.642 | 0.561    | 224   |
  | Jun   | 0.462   | 0.504  | 0.478 | 0.606 | 0.417    | 182   |
  | Jul   | 0.539   | 0.767  | 0.734 | 0.837 | 0.561    | 376   |
  | Aug   | 0.586   | 0.624  | 0.605 | 0.667 | 0.590    | 177   |
  | Sep   | 0.630   | 0.571  | 0.596 | 0.548 | 0.592    | 446   |
  | Oct   | 0.693   | 0.703  | 0.599 | 0.613 | 0.599    | 287   |
  | Nov   | 0.584   | 0.395  | 0.638 | 0.410 | 0.574    | 199   |
  | Dec   | 0.496   | 0.611  | 0.512 | 0.583 | 0.497    | 299   |
  |-------|---------|--------|-------|-------|----------|-------|
  | AVG   | 0.548   | 0.588  | 0.564 | 0.575 | 0.526    | 272   |

Summary Stats:
  - Best ROC-AUC: Oct (0.693)
  - Worst ROC-AUC: Jan (0.376)
  - High Volatility: 32-point swing (0.38 â†’ 0.69)

Notes:
  - Early stopping triggered at different epochs per month
  - July has unusual PR-AUC (0.767) vs ROC-AUC (0.539) = class imbalance
  - Validation holdout means 10% of most recent data NOT used for backprop
  - NEXT: Implement two-phase training (tune epochs on val, then retrain on all data)
```

---

### Experiment 007: OHLCV Market Context (Market Awareness)
- **Date**: 2025-12-17
- **Goal**: Integrate 60-day price history (Stock + SPY) to give the model "Market Context" (e.g. "Buying into a crash" vs "Buying into a rally").
- **Changes**:
    - **Data**: Added `data/price_sequences.pt` (60-day OHLCV for Stock + SPY, 10+10=20 features).
    - **Model**: Added `PriceEncoder` (LSTM) to TGN. Encodes 60x20 sequence -> 32-dim embedding.
    - **Integration**: Price Embedding concatenated to:
        - `Msg` (Memory Update): So memory remembers "context of the trade".
        - `EdgeAttr` (GNN): So neighbors share their context.
        - `Predictor` (Decoder): Explicit feature for the current prediction.
- **Hypothesis**: The model will learn to distinguish "smart buys" (buying dips) from "dumb buys" (buying tops), improving Precision.
- **Results (2023)**:
    - **Average ROC-AUC**: 0.584 (vs 0.596 Baseline) -> **Slight Drop**
    - **Average Macro-F1**: 0.499 (vs 0.569 Baseline) -> **Significant Drop**
    - **Observations**:
        - **Volatility**: Performance helps in some months (May: 0.705 vs 0.592) but hurts significantly in others (Jan-Apr).
        - **Instability**: Macro-F1 dropped, suggesting the LSTM might be introducing noise or leading to class collapse in specific months (e.g. May F1=0.14).
    - **Conclusion**: Naively attaching 60-day raw price sequences via LSTM did not yield immediate gains. The feature might be too noisy or requires better normalization (e.g. Returns instead of Log1p Price).

---

### Experiment 008: OHLCV Date Fix (Filing Date Basis)
- **Date**: 2025-12-17
- **Goal**: Fix the date basis for price sequences from Trade Date to Filing Date.
- **Changes**:
    - **Date Basis**: Changed `build_price_sequences.py` to use `Filed` date instead of `Traded` date.
    - **Rationale**: The market only learns about the trade on Filing Date. Using Trade Date was a form of look-ahead bias.
    - **Features**: Same OHLCV features as Exp 007 (no other changes).
- **Hypothesis**: Fixing the date basis will provide *relevant* market context (what the market looked like when the trade was disclosed), improving predictive power.
- **Results (2023)**:
    - **Average ROC-AUC**: 0.582 (vs Exp 007: 0.584, Baseline: 0.596)
    - **Average Macro-F1**: 0.526 (vs Exp 007: 0.499, Baseline: 0.569) -> **+5% vs Exp 007!**
    - **Highlights**:
        - May: 0.770 ROC-AUC, 0.701 Macro-F1 (Big improvement!)
        - December: 0.640 ROC-AUC, 0.593 Macro-F1 (Solid)
    - **Observations**:
        - Filing Date fix **improved stability** (Macro-F1 up 5% vs Exp 007).
        - Still below Baseline (0.596 / 0.569).
    - **Conclusion**: Date fix helped but OHLCV features still underperforming. Need better feature engineering (returns, RSI, etc.) to unlock value.

---

### Experiment 009: Engineered Features (Returns, RSI, Vol)
- **Date**: 2025-12-17
- **Goal**: Replace raw OHLCV sequences with engineered features for better signal extraction.
- **Changes**:
    - **Features (per asset)**: Return_1d, Return_5d, Return_10d, Return_20d, Volatility_20d, RSI_14, Vol_Ratio (7 features Ã— 2 assets = 14 total)
    - **Encoder**: Replaced LSTM with MLP (simpler, less overfitting)
    - **Date Basis**: Filing Date (same as Exp 008)
- **Hypothesis**: Engineered features are more informative and less noisy than raw prices, leading to better generalization.
- **Results (2023)**:
    - **Average ROC-AUC**: 0.589 (vs Exp 008: 0.582, Baseline: 0.596) -> **+0.007 vs Exp 008**
    - **Average Macro-F1**: 0.560 (vs Exp 008: 0.526, Baseline: 0.569) -> **Big Recovery!**
    - **Highlights**:
        - **November**: 0.703 Macro-F1 (vs 0.415 in Exp 008). Huge win.
        - **Stability**: Much less "class collapse" than previous attempts.


---

### Experiment 010: Feature Normalization & Regularization
- **Date**: 2025-12-17
- **Goal**: Fix feature scaling issues (Returns vs Volume) identified in Exp 009.
- **Changes**:
    - **Model**: Added `BatchNorm1d` (input normalization) and `Dropout(0.2)` to `PriceEncoder`.
    - **Features**: Same 14 engineered features as Exp 009.
- **Hypothesis**: Normalizing inputs allows the model to learn from small-scale features (like 0.01 daily returns) that were previously drowned out, improving F1 score.
- **Results (2023)**:
    - **Average ROC-AUC**: 0.592 (vs Exp 009: 0.589, Baseline: 0.596) -> **Step forward**
    - **Average Macro-F1**: 0.550 (vs Exp 009: 0.560, Baseline: 0.569) -> **Slight Drop**
    - **Highlights**:
        - **May**: 0.823 ROC-AUC, 0.721 Macro-F1! (Record High ðŸŒŸ)
        - **Oct/Nov**: Sustained high performance (>0.70).
    - **Conclusion**: Normalization definitely "amplified" the signal. May became super-predictable. However, it also amplified noise in bad months (Aug Macro-F1: 0.38).


---

### Experiment 011: Deep TGN (2-Layer GNN)
- **Date**: 2025-12-17
- **Goal**: Increase the model's receptive field from 1-hop (Direct Neighbors) to 2-hops (Friend-of-a-Friend).
- **Changes**:
    - **Architecture**: Replaced single `TransformerConv` with a 2-layer stack (`Conv` -> `ReLU` -> `Dropout` -> `Conv`).
    - **Context**: Allows model to see "indirect" market influences (e.g., if A trades similarly to B, and B trades effectively, A might too).
- **Hypothesis**: Deeper reasoning will improve classification in complex scenarios where direct signals are noisy.
- **Results (2023)**:
    - **Average ROC-AUC**: 0.597 (vs Exp 010: 0.592, Baseline: 0.596) -> **New Best!**
    - **Average Macro-F1**: 0.560 (vs Exp 010: 0.550, Baseline: 0.569) -> **Stable**
    - **Highlights**:
        - **Consistency**: The deeper model is generally more robust in AUC.
        - **May Regressed**: The "magic" peak from Exp 010 (AUC 0.82) dropped to 0.70. The 2-hop aggregation might have smoothed out the sharp signal that Exp 010 found.


---

### Experiment 012: Deep Interaction Decoder
- **Date**: 2025-12-17
- **Goal**: Allow model to learn non-linear interactions between Politician and Company features.
- **Changes**:
    - **Architecture**: Replaced `ElementWiseSum -> Relu -> Linear` decoder with `Concat(Src,Dst) -> MLP(296->128->64->1)`.
    - **Logic**: Instead of treating Politician and Company effects as separate additive factors, we allow them to *multiply/interact*.
    - **Dimensions**: Input per node = 148 (100 Memory + 16 Static + 32 Price). Pair = 296.
- **Hypothesis**: Specific politicians (e.g., Nancy Pelosi) may have unique "compatibilities" with specific sectors (e.g., Tech) that a simple sum cannot capture.
- **Results (2023)**:
    - **Average ROC-AUC**: 0.600 (vs Exp 011: 0.597) -> **Broken the 0.60 Barrier!** ðŸ†
    - **Average Macro-F1**: 0.563 (vs Exp 011: 0.560) -> **Slight Improvement**
    - **Highlights**:
        - **Consistency**: The combination of Deep GNN + Deep Decoder is the most robust configuration yet.
        - **Nov 2023**: 0.709 Macro-F1 (Very strong).
    - **Conclusion**: We have successfully upgraded every part of the model (Inputs, Encoder, GNN, Decoder). We are now consistently beating the baseline in AUC.


---

### Experiment 013: Wide TGN (Scaling Up)
- **Date**: 2025-12-18
- **Goal**: Give the Deep Architecture (2-Layer GNN + Deep Decoder) enough capacity to learn complex patterns.
- **Changes**:
    - **Dimensions**: `memory_dim` & `embedding_dim`: 100 -> **256** (2.56x larger)
    - **Attention**: `heads`: 4 -> **8** (More expressive attention)
    - **Regularization**: `dropout`: 0.1 -> **0.2** (To prevent overfitting with larger model)
- **Hypothesis**: The current ~100-dim model is too small for the deep architecture. Wider embeddings will allow the model to capture richer politician-stock interaction patterns.
- **Results (2023)**:
    - **Average ROC-AUC**: 0.592 (vs Exp 012: 0.600) -> **Regression (-0.008)**
    - **Average Macro-F1**: 0.561 (vs Exp 012: 0.563) -> **Slight Drop**
    - **Analysis**:
        - **Overfitting**: The larger model (256 dims) appears to be overfitting. Early months (Jan-Apr) show worse performance.
        - **Still Strong in Peak Months**: May (0.790 AUC) and Nov (0.746 AUC) remain strong.
    - **Conclusion**: Simply making the model wider did NOT help. The issue is not underfitting capacity - it's either:
        1. **Overtrain**: The wider model needs more regularization or fewer epochs.
        2. **Data Limitation**: We may have hit the ceiling of what can be learned from transaction data alone.
    - **Next Step**: Try advanced loss functions (Focal Loss) or class balancing to handle difficult samples better.






---

### Experiment 003: Two-Phase Training
```
Date: 2024-12-14 17:25
Changes:
  - Phase 1: Train 90%, find best epoch via early stopping
  - Phase 2: Reset model, retrain on 100% for best_epoch epochs
  - All other settings same as Exp 002
  
Config:
  - epochs=20 (max), patience=5
  - batch=200, lr=0.0001
  - loss=BCE (unweighted)
  - Two-phase: validation â†’ retrain full
  
Results (2023):
  | Month | ROC-AUC | PR-AUC | ACC   | F1    | Macro-F1 | Count |
  |-------|---------|--------|-------|-------|----------|-------|
  | Jan   | 0.579   | 0.658  | 0.553 | 0.602 | 0.546    | 331   |
  | Feb   | 0.442   | 0.442  | 0.470 | 0.548 | 0.453    | 230   |
  | Mar   | 0.547   | 0.554  | 0.570 | 0.662 | 0.536    | 342   |
  | Apr   | 0.516   | 0.546  | 0.519 | 0.663 | 0.412    | 370   |
  | May   | 0.676   | 0.740  | 0.652 | 0.690 | 0.646    | 224   |
  | Jun   | 0.609   | 0.586  | 0.555 | 0.509 | 0.551    | 182   |
  | Jul   | 0.804   | 0.868  | 0.766 | 0.855 | 0.622    | 376   |
  | Aug   | 0.543   | 0.574  | 0.537 | 0.573 | 0.533    | 177   |
  | Sep   | 0.500   | 0.450  | 0.500 | 0.494 | 0.500    | 446   |
  | Oct   | 0.724   | 0.734  | 0.693 | 0.732 | 0.687    | 287   |
  | Nov   | 0.650   | 0.540  | 0.533 | 0.513 | 0.532    | 199   |
  | Dec   | 0.629   | 0.714  | 0.692 | 0.796 | 0.587    | 299   |
  |-------|---------|--------|-------|-------|----------|-------|
  | AVG   | 0.601   | 0.617  | 0.587 | 0.636 | 0.550    | 272   |

Comparison vs Experiment 002:
  | Metric     | Exp 002 | Exp 003 | Delta  |
  |------------|---------|---------|--------|
  | Avg ROC-AUC| 0.548   | 0.601   | +0.053 â¬†ï¸ |
  | Avg PR-AUC | 0.588   | 0.617   | +0.029 â¬†ï¸ |
  | Avg ACC    | 0.564   | 0.587   | +0.023 â¬†ï¸ |
  | Avg F1     | 0.575   | 0.636   | +0.061 â¬†ï¸ |
  | Best Month | Oct(0.69)| Jul(0.80)| +0.11 â¬†ï¸ |
  | Worst Month| Jan(0.38)| Feb(0.44)| +0.06 â¬†ï¸ |

Notes:
  - TWO-PHASE TRAINING WORKS! All metrics improved.
  - Best month (July) now achieves 0.804 ROC-AUC (was 0.539 before)
  - Worst month improved from 0.376 to 0.442
  - Using all recent data for backprop clearly helps
  - NEXT: Try Weighted BCE/Focal Loss for class imbalance
```

---

### Experiment 004: Weighted BCE + MeanAggregator
```
Date: 2024-12-14 17:40
Changes:
  - Weighted BCE Loss (pos_weight = neg/pos ratio)
  - MeanAggregator (averages concurrent messages vs. LastAggregator)
  
Config:
  - epochs=20 (max), patience=5
  - batch=200, lr=0.0001
  - loss=Weighted BCE (dynamic per month)
  - Two-phase training
  
Results (2023):
  | Month | ROC-AUC | PR-AUC | ACC   | F1    | Macro-F1 | Count |
  |-------|---------|--------|-------|-------|----------|-------|
  | Jan   | 0.582   | 0.660  | 0.529 | 0.521 | 0.529    | 331   |
  | Feb   | 0.482   | 0.457  | 0.474 | 0.608 | 0.404    | 230   |
  | Mar   | 0.502   | 0.519  | 0.547 | 0.653 | 0.500    | 342   |
  | Apr   | 0.516   | 0.565  | 0.527 | 0.648 | 0.464    | 370   |
  | May   | 0.709   | 0.777  | 0.692 | 0.761 | 0.664    | 224   |
  | Jun   | 0.605   | 0.582  | 0.566 | 0.607 | 0.561    | 182   |
  | Jul   | 0.789   | 0.867  | 0.739 | 0.843 | 0.533    | 376   |
  | Aug   | 0.624   | 0.673  | 0.593 | 0.692 | 0.546    | 177   |
  | Sep   | 0.394   | 0.412  | 0.520 | 0.625 | 0.480    | 446   |
  | Oct   | 0.663   | 0.668  | 0.641 | 0.711 | 0.618    | 287   |
  | Nov   | 0.619   | 0.561  | 0.387 | 0.504 | 0.351    | 199   |
  | Dec   | 0.660   | 0.754  | 0.716 | 0.808 | 0.630    | 299   |
  |-------|---------|--------|-------|-------|----------|-------|
  | AVG   | 0.595   | 0.625  | 0.578 | 0.665 | 0.523    | 272   |

Comparison vs Experiment 003:
  | Metric     | Exp 003 | Exp 004 | Delta  |
  |------------|---------|---------|--------|
  | Avg ROC-AUC| 0.601   | 0.595   | -0.006 â¬‡ï¸ |
  | Avg PR-AUC | 0.617   | 0.625   | +0.008 â¬†ï¸ |
  | Avg ACC    | 0.587   | 0.578   | -0.009 â¬‡ï¸ |
  | Avg F1     | 0.636   | 0.665   | +0.029 â¬†ï¸ |
  | Best Month | Jul(0.80)| Jul(0.79)| -0.01 ~ |
  | Worst Month| Feb(0.44)| Sep(0.39)| -0.05 â¬‡ï¸ |

Notes:
  - MIXED RESULTS: F1 improved but ROC-AUC slightly declined
  - September collapsed (0.500 â†’ 0.394) - weighted BCE may overcorrect
  - December improved significantly (0.629 â†’ 0.660)
  - MeanAggregator effect unclear due to simultaneous BCE change
  - Macro-F1 decreased: weighted BCE may hurt class balance overall
  - NEXT: Consider reverting to unweighted BCE with just MeanAggregator
         OR try Focal Loss instead of weighted BCE
```

---

### Experiment 005: Focal Loss (Alpha=0.25, Gamma=2.0)
```
Date: 2024-12-17 14:15
Changes:
  - Replaced Weighted BCE with Focal Loss
  - Retained MeanAggregator and Two-Phase Training
  
Config:
  - epochs=20 (best), patience=5
  - Focal Loss: alpha=0.25, gamma=2.0
  
Results (2023):
  | Month | ROC-AUC | PR-AUC | ACC   | F1    | Macro-F1 | Count |
  |-------|---------|--------|-------|-------|----------|-------|
  | Jan   | 0.499   | 0.639  | 0.502 | 0.476 | 0.500    | 331   |
  | Feb   | 0.420   | 0.429  | 0.452 | 0.315 | 0.429    | 230   |
  | Mar   | 0.571   | 0.592  | 0.596 | 0.667 | 0.578    | 342   |
  | Apr   | 0.494   | 0.524  | 0.522 | 0.582 | 0.512    | 370   |
  | May   | 0.680   | 0.748  | 0.621 | 0.667 | 0.613    | 224   |
  | Jun   | 0.604   | 0.558  | 0.582 | 0.604 | 0.581    | 182   |
  | Jul   | 0.683   | 0.829  | 0.750 | 0.845 | 0.601    | 376   |
  | Aug   | 0.562   | 0.631  | 0.571 | 0.670 | 0.528    | 177   |
  | Sep   | 0.471   | 0.438  | 0.558 | 0.619 | 0.547    | 446   |
  | Oct   | 0.642   | 0.651  | 0.655 | 0.697 | 0.648    | 287   |
  | Nov   | 0.675   | 0.542  | 0.492 | 0.543 | 0.486    | 199   |
  | Dec   | 0.620   | 0.708  | 0.692 | 0.798 | 0.575    | 299   |
  |-------|---------|--------|-------|-------|----------|-------|
  | AVG   | 0.577   | 0.607  | 0.583 | 0.624 | 0.550    | 272   |

Comparison vs Experiment 003 (Best So Far):
  | Metric     | Exp 003 | Exp 005 | Delta  |
  |------------|---------|---------|--------|
  | Avg ROC-AUC| 0.601   | 0.577   | -0.024 â¬‡ï¸ |
  | Avg PR-AUC | 0.617   | 0.607   | -0.010 â¬‡ï¸ |
  | Avg ACC    | 0.587   | 0.583   | -0.004 â¬‡ï¸ |
  | Avg F1     | 0.636   | 0.624   | -0.012 â¬‡ï¸ |

Notes:
  - **Focal Loss FAILED** to improve over standard BCE (Exp 003).
  - Performance degraded across almost all metrics.
  - User noted: "Validation F1 is so small compared to Test F1".
  - Diagnosis: The validation split (last 10% of training) might not be representative, or the Focal Loss parameters (alpha=0.25) might be too aggressive for this dataset dynamic.
  - NEXT: 
    1. Revert to Exp 003 settings (Unweighted BCE).
    2. Investigate Validation/Test discrepancy.
```

---

### Experiment 006: Final Re-Evaluation (Validation of Exp 003)
```
Date: 2024-12-17 14:50
Changes:
  - Reverted to Exp 003 settings (Unweighted BCE)
  - Kept MeanAggregator (Exp 004 upgrade)
  - Full run to verify reproducibility and stability
  
Config:
  - same as Exp 003 but with MeanAggregator
  - epochs=20, patience=5
  - Two-Phase Training (Train+Val -> Retrain Full)
  
Results (2023):
  | Month | ROC-AUC | PR-AUC | ACC   | F1    | Macro-F1 | Count |
  |-------|---------|--------|-------|-------|----------|-------|
  | Jan   | 0.496   | 0.587  | 0.477 | 0.357 | 0.458    | 331   |
  | Feb   | 0.449   | 0.419  | 0.487 | 0.539 | 0.480    | 230   |
  | Mar   | 0.575   | 0.594  | 0.576 | 0.651 | 0.556    | 342   |
  | Apr   | 0.528   | 0.535  | 0.519 | 0.555 | 0.516    | 370   |
  | May   | 0.617   | 0.729  | 0.545 | 0.589 | 0.539    | 224   |
  | Jun   | 0.606   | 0.574  | 0.550 | 0.559 | 0.549    | 182   |
  | Jul   | 0.815   | 0.874  | 0.745 | 0.845 | 0.559    | 376   |
  | Aug   | 0.486   | 0.558  | 0.622 | 0.712 | 0.579    | 177   |
  | Sep   | 0.517   | 0.459  | 0.605 | 0.662 | 0.594    | 446   |
  | Oct   | 0.712   | 0.730  | 0.711 | 0.754 | 0.702    | 287   |
  | Nov   | 0.678   | 0.602  | 0.724 | 0.599 | 0.694    | 199   |
  | Dec   | 0.668   | 0.752  | 0.682 | 0.781 | 0.602    | 299   |
  |-------|---------|--------|-------|-------|----------|-------|
  | AVG   | 0.596   | 0.618  | 0.604 | 0.634 | 0.569    | 272   |

Conclusion:
  - **Reproducibility Confirmed**: Avg ROC-AUC (0.596) is very close to Exp 003 (0.601).
  - **July Signal is Real**: 0.815 ROC-AUC confirms the model found a strong signal in summer 2023.
  - **Improvement**: Macro-F1 (0.569) is better than Exp 003 (0.550) and Exp 004 (0.523).
  - **Verdict**: This configuration (Two-Phase + MeanAggregator + Unweighted BCE) is the stable baseline for Phase 2.
```

---

### Experiment 014: Date Logic Fix & Refined Ablation (Filed Date)
**Date**: 2026-01-25
**Changes**:
- **Date Basis**: Switched ALL logic from `Traded` date to `Filed` date.
  - **Reason**: To strictly prevent look-ahead bias. The model now only sees data available on the public filing date.
- **Refactoring**:
  - Moved scripts to `scripts/`.
  - Added `--full-only` flag to skip baseline ablations.
  - Dynamic result folders: `results/experiments/H_{horizon}_A_{alpha}/`.
  - JSON Reports: Renamed suffix `_flipped` to `_directional` for clarity.
**Config**:
- Same as Exp 013 but with `Filed` date basis.
**Verification**:
- Scripts execute correctly.
- Directory structure created as expected.
- Directional reports generated.

