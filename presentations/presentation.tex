\documentclass{beamer}
\usetheme{Madrid}
\usecolortheme{dolphin}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{xcolor}
\usetikzlibrary{shapes.geometric, arrows, positioning}

\title{Congressional Trading Prediction with Temporal Graph Networks}
\subtitle{Project Chocolate: End-to-End Development of a TGN-Based Copy Trading System}
\author{FinTech Project Team}
\date{\today}

\begin{document}

% ==============================================================================
% TITLE
% ==============================================================================
\begin{frame}
    \titlepage
\end{frame}

% ==============================================================================
% AGENDA
% ==============================================================================
\begin{frame}{Agenda}
    \tableofcontents
\end{frame}

% ==============================================================================
% SECTION 1: INTRODUCTION
% ==============================================================================
\section{Introduction \& Motivation}

\begin{frame}{Project Overview}
    \textbf{Objective:}
    Predict if copying a US Congressperson's stock trade will be significantly profitable (\textbf{1-month excess return vs SPY $> 6\%$}).
    
    \vspace{1em}
    \textbf{Why This Matters:}
    \begin{itemize}
        \item Congress members have \textbf{potential informational advantages} (committee access, briefings).
        \item Their trades are \textbf{publicly disclosed} (STOCK Act, 2012).
        \item Prior research shows some congress members significantly outperform the market.
    \end{itemize}
    
    \vspace{1em}
    \textbf{Core Challenge:}
    \begin{itemize}
        \item Trades are \textbf{temporal} (patterns shift over time).
        \item Politicians have \textbf{interconnected behaviors} (trading the same stocks).
        \item Traditional tabular ML ignores these relational dynamics.
    \end{itemize}
\end{frame}

\begin{frame}{Why Temporal Graph Networks (TGN)?}
    \textbf{Key Insight:} Model the market as a \textbf{dynamic graph} where:
    \begin{itemize}
        \item \textbf{Nodes} = Politicians ($\sim$500) and Stocks ($\sim$2,000).
        \item \textbf{Edges} = Transactions (Buy/Sell events at specific timestamps).
    \end{itemize}
    
    \vspace{1em}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \textbf{Traditional ML}
            \begin{itemize}
                \item Each trade is independent.
                \item No memory of past behavior.
                \item Static features only.
            \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \textbf{TGN Approach}
            \begin{itemize}
                \item Trades are \textbf{connected}.
                \item \textbf{Memory} per node (track record).
                \item \textbf{Dynamic} features evolving over time.
            \end{itemize}
        \end{column}
    \end{columns}
    
    \vspace{1em}
    \textbf{Result:} TGN can capture ``streaks'' and learn ``who'' trades ``what'' effectively.
\end{frame}

% ==============================================================================
% SECTION 2: MODEL EVOLUTION
% ==============================================================================
\section{Model Evolution (13 Experiments)}

\begin{frame}{Experiment Journey: From Baseline to Final Model}
    \textbf{We conducted 13 systematic experiments} to evolve the model:
    
    \vspace{0.5em}
    \begin{enumerate}
        \item \textbf{Exp 001-002}: Baseline TGN + Dynamic Label Masking
        \item \textbf{Exp 003}: Two-Phase Training (major breakthrough)
        \item \textbf{Exp 004-005}: Loss Function Experiments (Weighted BCE, Focal Loss)
        \item \textbf{Exp 006}: Validation of Best Configuration
        \item \textbf{Exp 007-010}: Market Context Features (OHLCV $\rightarrow$ Engineered)
        \item \textbf{Exp 011}: Deep TGN (2-Layer GNN)
        \item \textbf{Exp 012}: Deep Interaction Decoder
        \item \textbf{Exp 013}: Scaling Experiments
    \end{enumerate}
    
    \vspace{0.5em}
    \textbf{Key Insight:} Each component was tested independently to understand its contribution.
\end{frame}

\begin{frame}{Exp 001-003: Establishing the Foundation}
    \textbf{Exp 001: Baseline with Dynamic Labels}
    \begin{itemize}
        \item Added \texttt{Masked\_Label}: Historical trade outcome (Win/Lose) if resolved, else 0.5.
        \item Added \texttt{Age}: How old is each neighbor trade (log-normalized days).
        \item \textit{Example:} 1 day old $\rightarrow \ln(1+1) \approx 0.69$, 10 days $\rightarrow \ln(1+10) \approx 2.40$.
        \item \textbf{Result}: High volatility (AUC: 0.42 -- 0.81 across months).
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{Exp 002: Validation Split + Early Stopping}
    \begin{itemize}
        \item 90/10 chronological split for validation.
        \item \textbf{Problem}: Holding out 10\% of recent data hurt model quality.
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{Exp 003: Two-Phase Training} $\Leftarrow$ \textcolor{green}{\textbf{Major Breakthrough}}
    \begin{itemize}
        \item Phase 1: Find optimal epoch count on 90\% data.
        \item Phase 2: Retrain from scratch on 100\% data for that many epochs.
        \item \textbf{Result}: AUC 0.548 $\rightarrow$ \textbf{0.601 (+9.7\%)}
    \end{itemize}
\end{frame}

\begin{frame}{Exp 004-006: Loss Function Tuning}
    \textbf{Exp 004: Weighted BCE + MeanAggregator}
    \begin{itemize}
        \item Hypothesis: Class imbalance hurts performance.
        \item \textbf{Result}: Mixed -- F1 improved (+0.029), but AUC dropped (-0.006).
        \item September collapsed (AUC 0.39).
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{Exp 005: Focal Loss ($\alpha=0.25$, $\gamma=2.0$)}
    \begin{itemize}
        \item \textbf{Result}: \textcolor{red}{Failed}. All metrics degraded.
        \item Focal Loss too aggressive for this noisy dataset.
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{Exp 006: Validation Run}
    \begin{itemize}
        \item Reverted to Unweighted BCE + MeanAggregator.
        \item \textbf{Confirmed}: Two-Phase Training is the key driver.
        \item AUC: 0.596, Macro-F1: 0.569. \textcolor{green}{\textbf{Stable Baseline Established.}}
    \end{itemize}
\end{frame}

\begin{frame}{Exp 007-010: Adding Market Context}
    \textbf{Identified Gap:} ``The model has no access to market conditions.''
    
    \vspace{0.5em}
    \textbf{Exp 007: Raw OHLCV Sequences (60-day)}
    \begin{itemize}
        \item Added LSTM encoder for 60-day price history (Stock + SPY).
        \item \textbf{Result}: \textcolor{red}{Performance dropped}. LSTM introduced noise.
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{Exp 008: Filing Date Fix}
    \begin{itemize}
        \item Switched date basis from Trade Date to Filing Date.
        \item \textbf{Result}: Stability improved (Macro-F1 +5\%).
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{Exp 009-010: Engineered Features} $\Leftarrow$ \textcolor{green}{\textbf{Key Improvement}}
    \begin{itemize}
        \item Replaced LSTM with MLP on 14 engineered features:
        \item \texttt{Return\_1d, Return\_5d, Return\_10d, Return\_20d, Volatility\_20d, RSI\_14, Vol\_Ratio} (x2 for Stock + SPY).
        \item Added \texttt{BatchNorm1d} + \texttt{Dropout(0.2)}.
        \item \textbf{Result}: Macro-F1 recovered to 0.560.
    \end{itemize}
\end{frame}

\begin{frame}{Exp 011-012: Deepening the Architecture}
    \textbf{Exp 011: Deep TGN (2-Layer GNN)}
    \begin{itemize}
        \item Single TransformerConv $\rightarrow$ 2-Layer Stack with ReLU + Dropout.
        \item \textbf{Result}: AUC 0.597 (New Best). More robust across months.
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{Exp 012: Deep Interaction Decoder} $\Leftarrow$ \textcolor{green}{\textbf{Final Architecture}}
    \begin{itemize}
        \item Old: \texttt{Sum(Src, Dst)} $\rightarrow$ \texttt{ReLU} $\rightarrow$ \texttt{Linear(1)}.
        \item New: \texttt{Concat(Src, Dst)} $\rightarrow$ \texttt{MLP(296$\rightarrow$128$\rightarrow$64$\rightarrow$1)}.
        \item \textbf{Rationale}: Learn politician-stock \textit{interactions} (e.g., ``Pelosi + Tech'').
        \item \textbf{Result}: \textcolor{green}{\textbf{AUC 0.600}} -- Broke the 0.60 barrier!
    \end{itemize}
    
    \vspace{0.5em}
    \textbf{Exp 013: Scaling Up (256 dims, 8 heads)}
    \begin{itemize}
        \item \textbf{Result}: Overfitting. Performance regressed.
        \item \textbf{Conclusion}: Current capacity is sufficient.
    \end{itemize}
\end{frame}

% ==============================================================================
% SECTION 3: FINAL ARCHITECTURE
% ==============================================================================
\section{Final Architecture}

\begin{frame}{Graph Construction}
    \textbf{Data Representation: Continuous Temporal Bipartite Graph}
    
    \vspace{0.5em}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \textbf{Nodes:}
            \begin{itemize}
                \item \textbf{Politicians} (Source): $\sim$500 unique.
                \item \textbf{Stocks} (Destination): $\sim$2,000 unique.
            \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \textbf{Edges (Events):}
            \begin{itemize}
                \item Represent a single \texttt{BUY} or \texttt{SELL}.
                \item Timestamp: Filing Date.
                \item Directed: $Politician \rightarrow Stock$.
            \end{itemize}
        \end{column}
    \end{columns}
    
    \vspace{1em}
    \textbf{Dataset:}
    \begin{itemize}
        \item \textbf{Period:} 2012--2024 ($\sim$28,000 transactions).
        \item \textbf{Source:} Capitol Trades / House/Senate Stock Watcher.
        \item \textbf{Label:} Binary (1-Month \textbf{Post-Filing Excess Return} vs SPY $> 6\%$).
    \end{itemize}
\end{frame}

\begin{frame}{Feature Engineering (Complete)}
    \textbf{1. Dynamic Edge Features (per Transaction):}
    \begin{table}[h!]
        \centering
        \footnotesize
        \begin{tabular}{lp{6.5cm}}
            \toprule
            \textbf{Feature} & \textbf{Description} \\
            \midrule
            \texttt{Amount} & Log-normalized USD transaction size. \\
            \texttt{Is\_Buy} & Directional indicator (+1 Buy, -1 Sell). \\
            \texttt{Filing\_Gap} & Days between Trade and Disclosure. \\
            \texttt{Masked\_Label} & \textbf{Historical outcome} (Win/Lose) if resolved, else 0.5. \\
            \texttt{Age} & Log-normalized days since neighbor trade. \\
            \bottomrule
        \end{tabular}
    \end{table}
    
    \textbf{2. Static Node Features:} Party (8-dim), State (8-dim) embeddings.
    
    \vspace{0.3em}
    \textbf{3. Engineered Market Features (14 dims):}
    \begin{itemize}
        \item \texttt{Return\_1d, 5d, 10d, 20d}, \texttt{Volatility\_20d}, \texttt{RSI\_14}, \texttt{Vol\_Ratio}.
        \item Computed for both target stock and SPY (market benchmark).
    \end{itemize}
    
    \vspace{0.3em}
    \textbf{Key Anti-Leakage:} \texttt{Masked\_Label} only reveals resolved trade outcomes.
\end{frame}

\begin{frame}{TGN Architecture (Final)}
    \begin{enumerate}
        \item \textbf{Price Encoder (MLP):}
        \begin{itemize}
            \item Input: 14 engineered features $\rightarrow$ Output: 32-dim embedding.
            \item Includes \texttt{BatchNorm1d} + \texttt{Dropout(0.2)}.
        \end{itemize}
        
        \vspace{0.3em}
        \item \textbf{Memory Module (GRU):}
        \begin{itemize}
            \item Each node maintains hidden state $h(t)$ = 100-dim.
            \item Message: \texttt{[Amount, Is\_Buy, Gap, Price\_Emb]} (35-dim).
            \item Aggregator: \texttt{MeanAggregator} (handles concurrent trades).
        \end{itemize}
        
        \vspace{0.3em}
        \item \textbf{Graph Embedding (2-Layer TransformerConv):}
        \begin{itemize}
            \item 4 attention heads per layer, \texttt{Dropout(0.1)}.
            \item Edge features: \texttt{[Time\_Enc, Msg, Masked\_Label, Age]}.
            \item \textbf{Definition:} \texttt{Time\_Enc}: Relative time encoding ($t_{current} - t_{neighbor}$) to capture temporal distance.
        \end{itemize}
        
        \vspace{0.3em}
        \item \textbf{Deep Interaction Decoder (MLP):}
        \begin{itemize}
            \item Input: \texttt{Concat(Z\_src, Z\_dst, S\_src, S\_dst, P\_emb)} = 296-dim.
            \item \textbf{Definitions:}
            \begin{itemize}
                \item \texttt{Z}: Dynamic embeddings from graph (Politician, Stock).
                \item \texttt{S}: Static embeddings (Politician: Party/State, Stock: Placeholder).
                \item \texttt{P\_emb}: Market Price Embedding (14 engineered features).
            \end{itemize}
            \item Architecture: 296 $\rightarrow$ 128 $\rightarrow$ 64 $\rightarrow$ 1.
        \end{itemize}
    \end{enumerate}

\end{frame}

% ==============================================================================
% SECTION 4: EVALUATION PROTOCOL
% ==============================================================================
\section{Evaluation Protocol}

\begin{frame}{Expanding Window Evaluation (``Level 2'' Rigor)}
    \textbf{Goal:} Simulate real-world copy trading with no future information leakage.
    
    \vspace{0.5em}
    \textbf{For each Test Month:}
    \begin{enumerate}
        \item \textbf{Train:} All data from 2012 to (Test Month - 1 month).
        \item \textbf{Gap Phase:} Most recent month. Trades exist, but labels are \textbf{unknown}. Forward-only mode (update memory, no backprop).
        \item \textbf{Test:} Predict trades in the target month.
    \end{enumerate}
    
    \vspace{0.5em}
    \textbf{Two-Phase Training (per month):}
    \begin{enumerate}
        \item Phase 1: Train on 90\% of data, find best epoch via early stopping (patience=5).
        \item Phase 2: Retrain fresh model on 100\% data for best\_epoch epochs.
    \end{enumerate}
    
    \vspace{0.5em}
    \textbf{Metrics:} ROC-AUC, PR-AUC, F1-Score, Macro-F1.
\end{frame}

% ==============================================================================
% SECTION 5: ABLATION STUDY
% ==============================================================================
\section{Ablation Study}

\begin{frame}{Ablation Study: Which Signal Matters?}
    \textbf{Objective:} Isolate the contribution of each feature group over 6 years (2019--2024).
    
    \vspace{0.5em}
    \textbf{Configurations Tested:}
    \begin{enumerate}
        \item \textbf{Politician Signal Only (\texttt{pol\_only}):}
        \begin{itemize}
            \item Graph + Memory + Static Embeddings (Party, State).
            \item \textbf{Zero out} all 14 market features.
        \end{itemize}
        
        \item \textbf{Market Signal Only (\texttt{mkt\_only}):}
        \begin{itemize}
            \item 14 Engineered Market Features.
            \item \textbf{Zero out} Static Embeddings.
        \end{itemize}
        
        \item \textbf{Full Model (\texttt{full}):} All features active.
    \end{enumerate}
    
    \vspace{0.5em}
    \textbf{Scale:} 72 months $\times$ 3 modes = 216 model training runs.
\end{frame}

\begin{frame}{Ablation Results: Yearly F1 Score}
    \begin{table}[h!]
        \centering
        \small
        \begin{tabular}{lccc}
            \toprule
            \textbf{Year} & \textbf{Pol Only} & \textbf{Mkt Only} & \textbf{Full} \\
            \midrule
            2019 & 0.290 & 0.286 & 0.210 \\
            2020 & 0.487 & \textbf{0.538} & 0.484 \\
            2021 & 0.419 & \textbf{0.479} & 0.451 \\
            2022 & \textbf{0.523} & 0.429 & 0.503 \\
            2023 & \textbf{0.647} & 0.618 & 0.600 \\
            2024 & \textbf{0.673} & 0.664 & 0.645 \\
            \midrule
            \textbf{Mean} & 0.452 & \textbf{0.471} & 0.443 \\
            \bottomrule
        \end{tabular}
    \end{table}
    
    \textbf{Observation:} \texttt{mkt\_only} achieves highest F1 (0.471) -- market context helps \textbf{classification calibration}.
\end{frame}

\begin{frame}{Ablation Results: Yearly ROC-AUC}
    \begin{table}[h!]
        \centering
        \small
        \begin{tabular}{lccc}
            \toprule
            \textbf{Year} & \textbf{Pol Only} & \textbf{Mkt Only} & \textbf{Full} \\
            \midrule
            2019 & \textbf{0.535} & 0.501 & 0.523 \\
            2020 & 0.525 & \textbf{0.530} & 0.512 \\
            2021 & \textbf{0.583} & 0.570 & 0.564 \\
            2022 & \textbf{0.565} & 0.550 & 0.559 \\
            2023 & 0.599 & \textbf{0.609} & 0.593 \\
            2024 & 0.604 & \textbf{0.608} & 0.580 \\
            \midrule
            \textbf{Mean} & \textbf{0.560} & 0.553 & 0.551 \\
            \bottomrule
        \end{tabular}
    \end{table}
    
    \textbf{Observation:} \texttt{pol\_only} achieves highest AUC (0.560) -- politician identity is a strong \textbf{ranking signal}.
\end{frame}

\begin{frame}{Ablation Visualization (Trend)}
    \begin{figure}
        \centering
        \includegraphics[width=0.95\textwidth]{ablation_monthly_trend.png}
        \caption{Monthly F1 Trend (3-Month Rolling Average). All models improve from 2019 to 2024.}
    \end{figure}
\end{frame}

\begin{frame}{Ablation Visualization}
    \begin{figure}
        \centering
        \includegraphics[width=0.95\textwidth]{ablation_yearly_f1.png}
        \caption{Yearly Average F1 Score by Signal Source (2019--2024).}
    \end{figure}
\end{frame}

% ==============================================================================
% SECTION 6: DISCUSSION
% ==============================================================================
\section{Discussion \& Conclusions}

\begin{frame}{Key Findings}
    \begin{enumerate}
        \item \textbf{Two-Phase Training is Critical:}
        \begin{itemize}
            \item Exp 003 showed +9.7\% AUC improvement from this technique alone.
            \item Using all recent data for backprop is essential.
        \end{itemize}
        
        \vspace{0.5em}
        \item \textbf{Engineered Features > Raw Sequences:}
        \begin{itemize}
            \item LSTM on raw OHLCV failed (Exp 007).
            \item MLP on engineered features (RSI, Returns, Vol) succeeded (Exp 009-010).
        \end{itemize}
        
        \vspace{0.5em}
        \item \textbf{Politician Signal is Robust for Ranking:}
        \begin{itemize}
            \item Ablation: Highest AUC (0.560) using only graph + static embeddings.
        \end{itemize}
        
        \vspace{0.5em}
        \item \textbf{Full Model Shows No Clear Synergy:}
        \begin{itemize}
            \item Possible causes: Feature interference, overfitting, or suboptimal fusion.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}{Addressed Challenges \& Iterations}
    \textbf{We systematically addressed key modeling challenges:}
    
    \vspace{0.5em}
    \begin{table}[h!]
        \centering
        \footnotesize
        \begin{tabular}{lll}
            \toprule
            \textbf{Challenge} & \textbf{Status} & \textbf{Action} \\
            \midrule
            Memory underutilization & \textcolor{green}{Fixed} & Switched to MeanAggregator \\
            Missing market context & \textcolor{green}{Fixed} & Added 14 engineered features \\
            No validation set & \textcolor{green}{Fixed} & Two-phase training \\
            Class imbalance & \textcolor{orange}{Partial} & Tested Focal Loss (failed) \\
            Extreme volatility & \textcolor{orange}{Partial} & Improved but still present \\
            Stock-stock relationships & \textcolor{red}{Open} & Future work \\
            \bottomrule
        \end{tabular}
    \end{table}
\end{frame}

\begin{frame}{Limitations}
    \begin{itemize}
        \item \textbf{Label Resolution Delay:} 1-month labels are noisy (short-term volatility).
        \item \textbf{Sample Size Variance:} Monthly counts range from 100 to 600. Small samples cause high AUC variance.
        \item \textbf{Feature Fusion:} Concatenation may not be optimal. Attention-based fusion unexplored.
        \item \textbf{Graph Structure:} No stock-to-stock relationships (sector correlations not captured).
        \item \textbf{Survivorship Bias:} Only trades from active members included.
    \end{itemize}
\end{frame}

% ==============================================================================
% SECTION 7: FUTURE WORK
% ==============================================================================
\section{Future Work}

\begin{frame}{Future Directions: Enhancing the Graph}
    \begin{enumerate}
        \item \textbf{Stock-Stock Relationships:}
        \begin{itemize}
            \item Add sector similarity edges.
            \item Co-occurrence edges (stocks traded together).
        \end{itemize}
        
        \vspace{0.5em}
        \item \textbf{Politician Social Graph:}
        \begin{itemize}
            \item Add \textbf{Politician-Politician edges} based on shared committee memberships.
            \item Tests hypothesis: ``Do committee peers trade similarly?''
        \end{itemize}
        
        \vspace{0.5em}
        \item \textbf{Dynamic Political Affiliation:}
        \begin{itemize}
            \item Currently, Party is static (first observed).
            \item Future: Attach Party as an \textbf{edge feature} to capture party switching or committee changes over time.
        \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}{Future Directions: Model \& Strategy}
    \begin{enumerate}
        \item \textbf{Advanced Fusion Mechanisms:}
        \begin{itemize}
            \item Cross-Attention between Static/Market branches.
            \item Gated Fusion Networks.
        \end{itemize}
        
        \vspace{0.5em}
        \item \textbf{Politician Success Scores (Dynamic Feature):}
        \begin{itemize}
            \item Historical win-rate per politician (updated monthly).
            \item Leverage \texttt{BioGuideID} to track long-term performance.
        \end{itemize}
        
        \vspace{0.5em}
        \item \textbf{Copy-Trading Strategy Backtesting:}
        \begin{itemize}
            \item Simulate portfolio returns based on model predictions.
            \item Metrics: Sharpe Ratio, Max Drawdown, Calmar Ratio.
        \end{itemize}
        
        \vspace{0.5em}
        \item \textbf{Memory Decay Mechanism:}
        \begin{itemize}
            \item Implement time-based decay for old memory states ($h(t)$).
            \item Reduce influence of outdated trading patterns (e.g., > 2 years).
        \end{itemize}
    \end{enumerate}
\end{frame}

% ==============================================================================
% THANK YOU
% ==============================================================================
\begin{frame}
    \centering
    \Huge \textbf{Thank You!}
    
    \vspace{2em}
    \normalsize
    Questions?
    
    \vspace{1em}
    \textbf{Project Directory:} \texttt{/fintech/chocolate}
\end{frame}

\end{document}
